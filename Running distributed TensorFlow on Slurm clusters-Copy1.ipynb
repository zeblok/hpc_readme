{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Distributed TensorFlow on Slurm Clusters\n",
    "\n",
    "In this notebook, we provide an example of how to run a TensorFlow experiment on a Slurm cluster. Since TensorFlow doesn't yet officailly support this task, we developed a simple Python module for automating the configuration. It parses the environment variables set by Slurm and creates a TensorFlow cluster configuration based on them. We are running this notebook with a simple image recognition example on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slurm_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext slurm_magic\n"
     ]
    }
   ],
   "source": [
    "# load module slurm\n",
    "%load_ext slurm_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1  Create conda environment\n",
    "Create a new conda environment based off Python 3 (currently 3.7 for TensorFlow 1.15 and 2.0, or 3.6 for TensorFlow 1.14). Name the environment whatever you want, here we name it new_conda_env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "base                  *  /opt/conda\r\n",
      "tf2-gpu                  /opt/conda/envs/tf2-gpu\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# First, check the conda environment list.\n",
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the name you difined has been used, you can choose to remove it\n",
    "!conda env remove -p /lustre/-42ai/work/new_conda_env \n",
    "# Or change the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /lustre/-42ai/work/new_conda_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - tensorflow-gpu\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    absl-py-0.9.0              |   py37hc8dfbb8_1         162 KB  conda-forge\n",
      "    google-pasta-0.2.0         |     pyh8c360ce_0          42 KB  conda-forge\n",
      "    grpcio-1.30.0              |   py37hb0870dc_0         2.0 MB  conda-forge\n",
      "    h5py-2.10.0                |nompi_py37h90cd8ad_103         1.1 MB  conda-forge\n",
      "    hdf5-1.10.6                |nompi_h3c11f04_100         3.0 MB  conda-forge\n",
      "    importlib-metadata-1.7.0   |   py37hc8dfbb8_0          44 KB  conda-forge\n",
      "    keras-applications-1.0.8   |             py_1          30 KB  conda-forge\n",
      "    keras-preprocessing-1.1.0  |             py_0          33 KB  conda-forge\n",
      "    libopenblas-0.3.10         |pthreads_hb3c22a3_2         7.8 MB  conda-forge\n",
      "    ncurses-6.2                |       he1b5a44_1         993 KB  conda-forge\n",
      "    numpy-1.19.0               |   py37h8960a57_0         5.2 MB  conda-forge\n",
      "    protobuf-3.12.3            |   py37h3340039_0         702 KB  conda-forge\n",
      "    readline-8.0               |       he28a2e2_2         281 KB  conda-forge\n",
      "    scipy-1.5.1                |   py37ha3d9a3c_0        18.5 MB  conda-forge\n",
      "    setuptools-49.2.0          |   py37hc8dfbb8_0         914 KB  conda-forge\n",
      "    tensorboard-1.14.0         |           py37_0         3.2 MB  conda-forge\n",
      "    tensorflow-1.14.0          |       h4531e10_0          23 KB  conda-forge\n",
      "    tensorflow-base-1.14.0     |   py37h4531e10_0        87.6 MB  conda-forge\n",
      "    tensorflow-estimator-1.14.0|   py37h5ca1d4c_0         645 KB  conda-forge\n",
      "    tensorflow-gpu-1.14.0      |       h0d30ee6_0           3 KB  defaults\n",
      "    wrapt-1.12.1               |   py37h8f50634_1          46 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       132.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-0_gnu\n",
      "  _tflow_select      pkgs/main/linux-64::_tflow_select-2.1.0-gpu\n",
      "  absl-py            conda-forge/linux-64::absl-py-0.9.0-py37hc8dfbb8_1\n",
      "  astor              conda-forge/noarch::astor-0.8.1-pyh9f0ad1d_0\n",
      "  c-ares             conda-forge/linux-64::c-ares-1.15.0-h516909a_1001\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates-2020.6.20-hecda079_0\n",
      "  certifi            conda-forge/linux-64::certifi-2020.6.20-py37hc8dfbb8_0\n",
      "  gast               conda-forge/noarch::gast-0.3.3-py_0\n",
      "  google-pasta       conda-forge/noarch::google-pasta-0.2.0-pyh8c360ce_0\n",
      "  grpcio             conda-forge/linux-64::grpcio-1.30.0-py37hb0870dc_0\n",
      "  h5py               conda-forge/linux-64::h5py-2.10.0-nompi_py37h90cd8ad_103\n",
      "  hdf5               conda-forge/linux-64::hdf5-1.10.6-nompi_h3c11f04_100\n",
      "  importlib-metadata conda-forge/linux-64::importlib-metadata-1.7.0-py37hc8dfbb8_0\n",
      "  keras-applications conda-forge/noarch::keras-applications-1.0.8-py_1\n",
      "  keras-preprocessi~ conda-forge/noarch::keras-preprocessing-1.1.0-py_0\n",
      "  krb5               conda-forge/linux-64::krb5-1.17.1-hfafb76e_1\n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.34-h53a641e_7\n",
      "  libblas            conda-forge/linux-64::libblas-3.8.0-17_openblas\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.8.0-17_openblas\n",
      "  libcurl            conda-forge/linux-64::libcurl-7.71.1-hcdd3856_1\n",
      "  libedit            conda-forge/linux-64::libedit-3.1.20191231-h46ee950_1\n",
      "  libffi             conda-forge/linux-64::libffi-3.2.1-he1b5a44_1007\n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-9.2.0-h24d8f2e_2\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-hdf63c60_6\n",
      "  libgomp            conda-forge/linux-64::libgomp-9.2.0-h24d8f2e_2\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.8.0-17_openblas\n",
      "  libopenblas        conda-forge/linux-64::libopenblas-0.3.10-pthreads_hb3c22a3_2\n",
      "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
      "  libprotobuf        conda-forge/linux-64::libprotobuf-3.12.3-h8b12597_1\n",
      "  libssh2            conda-forge/linux-64::libssh2-1.9.0-hab1572f_3\n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-9.2.0-hdf63c60_2\n",
      "  markdown           conda-forge/noarch::markdown-3.2.2-py_0\n",
      "  ncurses            conda-forge/linux-64::ncurses-6.2-he1b5a44_1\n",
      "  numpy              conda-forge/linux-64::numpy-1.19.0-py37h8960a57_0\n",
      "  openssl            conda-forge/linux-64::openssl-1.1.1g-h516909a_0\n",
      "  pip                conda-forge/noarch::pip-20.1.1-py_1\n",
      "  protobuf           conda-forge/linux-64::protobuf-3.12.3-py37h3340039_0\n",
      "  python             conda-forge/linux-64::python-3.7.6-cpython_h8356626_6\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
      "  readline           conda-forge/linux-64::readline-8.0-he28a2e2_2\n",
      "  scipy              conda-forge/linux-64::scipy-1.5.1-py37ha3d9a3c_0\n",
      "  setuptools         conda-forge/linux-64::setuptools-49.2.0-py37hc8dfbb8_0\n",
      "  six                conda-forge/noarch::six-1.15.0-pyh9f0ad1d_0\n",
      "  sqlite             conda-forge/linux-64::sqlite-3.32.3-hcee41ef_1\n",
      "  tensorboard        conda-forge/linux-64::tensorboard-1.14.0-py37_0\n",
      "  tensorflow         conda-forge/linux-64::tensorflow-1.14.0-h4531e10_0\n",
      "  tensorflow-base    conda-forge/linux-64::tensorflow-base-1.14.0-py37h4531e10_0\n",
      "  tensorflow-estima~ conda-forge/linux-64::tensorflow-estimator-1.14.0-py37h5ca1d4c_0\n",
      "  tensorflow-gpu     pkgs/main/linux-64::tensorflow-gpu-1.14.0-h0d30ee6_0\n",
      "  termcolor          conda-forge/noarch::termcolor-1.1.0-py_2\n",
      "  tk                 conda-forge/linux-64::tk-8.6.10-hed695b0_0\n",
      "  werkzeug           conda-forge/noarch::werkzeug-1.0.1-pyh9f0ad1d_0\n",
      "  wheel              conda-forge/noarch::wheel-0.34.2-py_1\n",
      "  wrapt              conda-forge/linux-64::wrapt-1.12.1-py37h8f50634_1\n",
      "  xz                 conda-forge/linux-64::xz-5.2.5-h516909a_1\n",
      "  zipp               conda-forge/noarch::zipp-3.1.0-py_0\n",
      "  zlib               conda-forge/linux-64::zlib-1.2.11-h516909a_1006\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "tensorflow-base-1.14 | 87.6 MB   | ##################################### | 100% \n",
      "wrapt-1.12.1         | 46 KB     | ##################################### | 100% \n",
      "keras-preprocessing- | 33 KB     | ##################################### | 100% \n",
      "tensorflow-gpu-1.14. | 3 KB      | ##################################### | 100% \n",
      "readline-8.0         | 281 KB    | ##################################### | 100% \n",
      "keras-applications-1 | 30 KB     | ##################################### | 100% \n",
      "tensorflow-estimator | 645 KB    | ##################################### | 100% \n",
      "libopenblas-0.3.10   | 7.8 MB    | ##################################### | 100% \n",
      "ncurses-6.2          | 993 KB    | ##################################### | 100% \n",
      "importlib-metadata-1 | 44 KB     | ##################################### | 100% \n",
      "numpy-1.19.0         | 5.2 MB    | ##################################### | 100% \n",
      "h5py-2.10.0          | 1.1 MB    | ##################################### | 100% \n",
      "protobuf-3.12.3      | 702 KB    | ##################################### | 100% \n",
      "grpcio-1.30.0        | 2.0 MB    | ##################################### | 100% \n",
      "absl-py-0.9.0        | 162 KB    | ##################################### | 100% \n",
      "google-pasta-0.2.0   | 42 KB     | ##################################### | 100% \n",
      "tensorflow-1.14.0    | 23 KB     | ##################################### | 100% \n",
      "scipy-1.5.1          | 18.5 MB   | ##################################### | 100% \n",
      "hdf5-1.10.6          | 3.0 MB    | ##################################### | 100% \n",
      "tensorboard-1.14.0   | 3.2 MB    | ##################################### | 100% \n",
      "setuptools-49.2.0    | 914 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /lustre/-42ai/work/new_conda_env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# create conda environment new_conda_env with package TensorFlow\n",
    "!yes | conda create --prefix /lustre/-42ai/work/new_conda_env tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Testing of the Tensorflow installation\n",
    "\n",
    "Now we write a python file test_tf.py to check if TensorFlow installed successfully, and then we print out the version of TensorFlow we installed.\n",
    "\n",
    "Remember to activate the conda envirionment in your slurm script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/test_tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/test_tf.py\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/test_tf.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/test_tf.sh\n",
    "#! /bin/bash\n",
    "#SBATCH --output=test_tf.out              # Name of stdout output file\n",
    "#SBATCH --error=test_tf.err             # Name of stderr error file\n",
    "#SBATCH --nodes=1                    # Total # of nodes\n",
    "#SBATCH --ntasks=1                   # Total # of mpi tasks\n",
    "#SBATCH --partition=debug                # Queue (partition) name\n",
    "#SBATCH --ntasks-per-node  1   # Number of tasks per node\n",
    "#SBATCH --time=12:00:00            # Run time (hh:mm:ss)\n",
    "\n",
    "source activate /lustre/-42ai/work/new_conda_env \n",
    "echo \"SLURM_JOB_ID $SLURM_JOB_ID  SLURM_JOB_NAME  $SLURM_JOB_NAME SLURM_JOB_NODELIST  $SLURM_JOB_NODELIST  SLURMD_NODENAME $SLURMD_NODENAME   SLURM_JOB_NUM_NODES  $SLURM_JOB_NUM_NODES\";\n",
    "python /lustre/-42ai/work/test_tf.py\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submitted batch job 138\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit the job\n",
    "%sbatch -D /lustre/-42ai/work /lustre/-42ai/work/test_tf.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID 138  SLURM_JOB_NAME  test_tf.sh SLURM_JOB_NODELIST  slurm-worker-8  SLURMD_NODENAME slurm-worker-8   SLURM_JOB_NUM_NODES  1\r\n",
      "1.14.0\r\n",
      "b'Hello, TensorFlow!'\r\n",
      "Done!\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/test_tf.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not see \"Done!\" in the output of the following cell, please wiat for a moment and rerun the following cell untill the it contains \"Done!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the library list in your conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /lustre/-42ai/work/new_conda_env:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n",
      "_openmp_mutex             4.5                       0_gnu    conda-forge\r\n",
      "_tflow_select             2.1.0                       gpu    defaults\r\n",
      "absl-py                   0.9.0            py37hc8dfbb8_1    conda-forge\r\n",
      "astor                     0.8.1              pyh9f0ad1d_0    conda-forge\r\n",
      "c-ares                    1.15.0            h516909a_1001    conda-forge\r\n",
      "ca-certificates           2020.6.20            hecda079_0    conda-forge\r\n",
      "certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge\r\n",
      "gast                      0.3.3                      py_0    conda-forge\r\n",
      "google-pasta              0.2.0              pyh8c360ce_0    conda-forge\r\n",
      "grpcio                    1.30.0           py37hb0870dc_0    conda-forge\r\n",
      "h5py                      2.10.0          nompi_py37h90cd8ad_103    conda-forge\r\n",
      "hdf5                      1.10.6          nompi_h3c11f04_100    conda-forge\r\n",
      "importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge\r\n",
      "keras-applications        1.0.8                      py_1    conda-forge\r\n",
      "keras-preprocessing       1.1.0                      py_0    conda-forge\r\n",
      "krb5                      1.17.1               hfafb76e_1    conda-forge\r\n",
      "ld_impl_linux-64          2.34                 h53a641e_7    conda-forge\r\n",
      "libblas                   3.8.0               17_openblas    conda-forge\r\n",
      "libcblas                  3.8.0               17_openblas    conda-forge\r\n",
      "libcurl                   7.71.1               hcdd3856_1    conda-forge\r\n",
      "libedit                   3.1.20191231         h46ee950_1    conda-forge\r\n",
      "libffi                    3.2.1             he1b5a44_1007    conda-forge\r\n",
      "libgcc-ng                 9.2.0                h24d8f2e_2    conda-forge\r\n",
      "libgfortran-ng            7.5.0                hdf63c60_6    conda-forge\r\n",
      "libgomp                   9.2.0                h24d8f2e_2    conda-forge\r\n",
      "liblapack                 3.8.0               17_openblas    conda-forge\r\n",
      "libopenblas               0.3.10          pthreads_hb3c22a3_2    conda-forge\r\n",
      "libpng                    1.6.37               hed695b0_1    conda-forge\r\n",
      "libprotobuf               3.12.3               h8b12597_1    conda-forge\r\n",
      "libssh2                   1.9.0                hab1572f_3    conda-forge\r\n",
      "libstdcxx-ng              9.2.0                hdf63c60_2    conda-forge\r\n",
      "markdown                  3.2.2                      py_0    conda-forge\r\n",
      "ncurses                   6.2                  he1b5a44_1    conda-forge\r\n",
      "numpy                     1.19.0           py37h8960a57_0    conda-forge\r\n",
      "openssl                   1.1.1g               h516909a_0    conda-forge\r\n",
      "pip                       20.1.1                     py_1    conda-forge\r\n",
      "protobuf                  3.12.3           py37h3340039_0    conda-forge\r\n",
      "python                    3.7.6           cpython_h8356626_6    conda-forge\r\n",
      "python_abi                3.7                     1_cp37m    conda-forge\r\n",
      "readline                  8.0                  he28a2e2_2    conda-forge\r\n",
      "scipy                     1.5.1            py37ha3d9a3c_0    conda-forge\r\n",
      "setuptools                49.2.0           py37hc8dfbb8_0    conda-forge\r\n",
      "six                       1.15.0             pyh9f0ad1d_0    conda-forge\r\n",
      "sqlite                    3.32.3               hcee41ef_1    conda-forge\r\n",
      "tensorboard               1.14.0                   py37_0    conda-forge\r\n",
      "tensorflow                1.14.0               h4531e10_0    conda-forge\r\n",
      "tensorflow-base           1.14.0           py37h4531e10_0    conda-forge\r\n",
      "tensorflow-estimator      1.14.0           py37h5ca1d4c_0    conda-forge\r\n",
      "tensorflow-gpu            1.14.0               h0d30ee6_0    defaults\r\n",
      "termcolor                 1.1.0                      py_2    conda-forge\r\n",
      "tk                        8.6.10               hed695b0_0    conda-forge\r\n",
      "werkzeug                  1.0.1              pyh9f0ad1d_0    conda-forge\r\n",
      "wheel                     0.34.2                     py_1    conda-forge\r\n",
      "wrapt                     1.12.1           py37h8f50634_1    conda-forge\r\n",
      "xz                        5.2.5                h516909a_1    conda-forge\r\n",
      "zipp                      3.1.0                      py_0    conda-forge\r\n",
      "zlib                      1.2.11            h516909a_1006    conda-forge\r\n"
     ]
    }
   ],
   "source": [
    "!conda list -p /lustre/-42ai/work/new_conda_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check Slurm Nodes\n",
    "\n",
    "When running a Slurm job we can discover other nodes taking part by examining evnironment variables:\n",
    "\n",
    "SLURMD_NODENAME – name of the current node\n",
    "\n",
    "\n",
    "SLURM_JOB_NODELIST – number of nodes the job is using\n",
    "\n",
    "SLURM_JOB_NUM_NODES – list of all nodes allocated to the job\n",
    "\n",
    "let's see this example, the output will give us this job's ID, name, nodelist and other infomation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /lustre/-42ai/work/check_slurm.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/check_slurm.sh\n",
    "#! /bin/bash\n",
    "#SBATCH --output=check_slurm.out              # Name of stdout output file\n",
    "#SBATCH --error=check_slurm.err             # Name of stderr error file\n",
    "#SBATCH --nodes=1                    # Total # of nodes\n",
    "#SBATCH --ntasks=1                   # Total # of mpi tasks\n",
    "#SBATCH --partition=debug                # Queue (partition) name\n",
    "#SBATCH --ntasks-per-node  1   # Number of tasks per node\n",
    "#SBATCH --time=12:00:00            # Run time (hh:mm:ss)\n",
    "\n",
    "source activate /lustre/-42ai/work/new_conda_env \n",
    "echo \"SLURM_JOB_ID $SLURM_JOB_ID  SLURM_JOB_NAME  $SLURM_JOB_NAME SLURM_JOB_NODELIST  $SLURM_JOB_NODELIST  SLURMD_NODENAME $SLURMD_NODENAME   SLURM_JOB_NUM_NODES  $SLURM_JOB_NUM_NODES\";\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submitted batch job 139\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit the job\n",
    "%sbatch -D /lustre/-42ai/work /lustre/-42ai/work/check_slurm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID 139  SLURM_JOB_NAME  check_slurm.sh SLURM_JOB_NODELIST  slurm-worker-8  SLURMD_NODENAME slurm-worker-8   SLURM_JOB_NUM_NODES  1\r\n",
      "Done!\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/check_slurm.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing of reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/lustre/-42ai/work/example’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir /lustre/-42ai/work/example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --directory-prefix=/lustre/-42ai/work/example \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvzf /lustre/-42ai/work/example/cifar-10-python.tar.gz --directory /lustre/-42ai/work/example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta  data_batch_2  data_batch_4  readme.html\r\n",
      "data_batch_1  data_batch_3  data_batch_5  test_batch\r\n"
     ]
    }
   ],
   "source": [
    "!ls /lustre/-42ai/work/example/cifar-10-batches-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/example/check_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  /lustre/-42ai/work/example/check_dataset.py\n",
    "\n",
    "# check dataset load successfully\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/lustre/-42ai/work/example/cifar-10-batches-py'\n",
    "# try the first two batches in this example\n",
    "filelist = [os.path.join(data_dir, 'data_batch_1'),\n",
    "            os.path.join(data_dir, 'data_batch_2')]\n",
    "data, labels = [], []\n",
    "\n",
    "for f in filelist:\n",
    "    with open(f, 'rb') as fo:\n",
    "        data_elem = pickle.load(fo, encoding='latin1')\n",
    "        data.append(data_elem['data'])\n",
    "        labels.extend(data_elem['labels'])\n",
    "data = np.vstack(d for d in data)\n",
    "print('data shape: ', data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/check_dataset.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/check_dataset.sh\n",
    "#! /bin/bash\n",
    "#SBATCH --output=check_dataset.out              # Name of stdout output file\n",
    "#SBATCH --error=check_dataset.error             # Name of stderr error file\n",
    "#SBATCH --nodes=1                    # Total # of nodes\n",
    "#SBATCH --ntasks=1                   # Total # of mpi tasks\n",
    "#SBATCH --partition=debug                # Queue (partition) name\n",
    "#SBATCH --ntasks-per-node  1   # Number of tasks per node\n",
    "#SBATCH --time=12:00:00            # Run time (hh:mm:ss)\n",
    "\n",
    "source activate /lustre/-42ai/work/new_conda_env \n",
    "echo \"SLURM_JOB_ID $SLURM_JOB_ID  SLURM_JOB_NAME  $SLURM_JOB_NAME SLURM_JOB_NODELIST  $SLURM_JOB_NODELIST  SLURMD_NODENAME $SLURMD_NODENAME   SLURM_JOB_NUM_NODES  $SLURM_JOB_NUM_NODES\";\n",
    "python /lustre/-42ai/work/example/check_dataset.py\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submitted batch job 140\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit the job\n",
    "%sbatch -D /lustre/-42ai/work /lustre/-42ai/work/check_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID 140  SLURM_JOB_NAME  check_dataset.sh SLURM_JOB_NODELIST  slurm-worker-8  SLURMD_NODENAME slurm-worker-8   SLURM_JOB_NUM_NODES  1\r\n",
      "data shape:  (20000, 3072)\r\n",
      "Done!\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/check_dataset.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Start server\n",
    "\n",
    "First we are going to define the cluster spec which is necessary to crete the server along with the task name and ask index of the current job. The 'ps_number' parameter specifies how many parameter servers to set up(we use 1). All other nodes will be working as normal workers and everything gets passed to the tf.distribute.Server constructor.\n",
    "\n",
    "\n",
    "\n",
    "Pay a attention that Do Not Run the following two cells repetedly, Because if you have started the server and did not kill it, you cannot use that port again. It will give you error \"UnknownError: Could not start gRPC server\". Restart the kernel can solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  /lustre/-42ai/work/example/train.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "import traceback\n",
    "try:\n",
    "\n",
    "    def _pad_zeros(iterable, length):\n",
    "        return (str(t).rjust(length, '0') for t in iterable)\n",
    "\n",
    "    def _expand_ids(ids):\n",
    "        ids = ids.split(',')\n",
    "        result = []\n",
    "        for id in ids:\n",
    "            if '-' in id:\n",
    "                begin, end = [int(token) for token in id.split('-')]\n",
    "                result.extend(_pad_zeros(range(begin, end+1), len( id.split('-')[0] )))\n",
    "            else:\n",
    "                result.append(id)\n",
    "        return result\n",
    "\n",
    "    def _expand_nodelist(nodelist):\n",
    "        prefix, ids = re.findall(\"(.*)\\[(.*)\\]\", nodelist)[0]\n",
    "        ids = _expand_ids(ids)\n",
    "        result = [prefix + str(id) for id in ids]\n",
    "        return result\n",
    "\n",
    "    nodename = os.environ[\"SLURMD_NODENAME\"]\n",
    "    nodelist = os.environ[\"SLURM_JOB_NODELIST\"]\n",
    "    nodelist = _expand_nodelist(nodelist)\n",
    "    num_nodes = int(os.getenv(\"SLURM_JOB_NUM_NODES\"))\n",
    "\n",
    "    if len(nodelist) != num_nodes:\n",
    "        raise ValueError(\"Number of slurm nodes {} not equal to {}\".format(len(nodelist), num_nodes))\n",
    "\n",
    "    if nodename not in nodelist:\n",
    "        raise ValueError(\"Nodename({}) not in nodelist({}). This should not happen! \".format(nodename,nodelist))\n",
    "except Exception:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "try:\n",
    "    ps_number = 1\n",
    "\n",
    "    ps_nodes = [node for i, node in enumerate(nodelist) if i < ps_number]\n",
    "    worker_nodes = [node for i, node in enumerate(nodelist) if i >= ps_number]\n",
    "\n",
    "    print(\"Parameter nodes: \", ps_nodes)\n",
    "    print(\"Worker nodes: \", worker_nodes)\n",
    "except Exception:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "try: \n",
    "    if nodename in ps_nodes:\n",
    "        my_job_name = \"ps\"\n",
    "        my_task_index = ps_nodes.index(nodename)\n",
    "    else:\n",
    "        my_job_name = \"worker\"\n",
    "        my_task_index = worker_nodes.index(nodename)\n",
    "\n",
    "    print(\"My job name: \", my_job_name)\n",
    "    print(\"My task index: \", my_task_index)\n",
    "except Exception:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "try:\n",
    "    port_number = 2222\n",
    "\n",
    "    worker_sockets = [\":\".join([node, str(port_number)]) for node in worker_nodes]\n",
    "    ps_sockets = [\":\".join([node, str(port_number)]) for node in ps_nodes]\n",
    "\n",
    "    cluster = {\"worker\": worker_sockets, \"ps\" : ps_sockets}\n",
    "\n",
    "    print(\"Cluster: \", cluster)\n",
    "except Exception:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "try:\n",
    "    cluster_spec = tf.train.ClusterSpec(cluster)\n",
    "    server = tf.train.Server(server_or_cluster_def=cluster_spec,\n",
    "                             job_name=my_job_name,\n",
    "                             task_index=my_task_index)\n",
    "except Exception:\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check my cluster, my index name, my job name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/example/job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/example/job.sh\n",
    "#! /bin/bash\n",
    "#SBATCH --output=example_1.out              # Name of stdout output file\n",
    "#SBATCH --error=example_1.err             # Name of stderr error file\n",
    "#SBATCH --nodes=5                    # Total # of nodes\n",
    "#SBATCH --ntasks=5                   # Total # of mpi tasks\n",
    "#SBATCH --partition=debug                # Queue (partition) name\n",
    "#SBATCH --ntasks-per-node  1   # Number of tasks per node\n",
    "#SBATCH --mem=4G                 # total memory per node (default is 4 GB per CPU-core)\n",
    "#SBATCH --gres=gpu:1             # number of gpus per node\n",
    "#SBATCH --time=00:01:00            # Run time (hh:mm:ss)\n",
    "source activate /lustre/-42ai/work/new_conda_env\n",
    "\n",
    "echo \"SLURM_JOB_ID \" $SLURM_JOB_ID  \"; SLURM_JOB_NAME \" $SLURM_JOB_NAME \"; SLURM_JOB_NODELIST \" $SLURM_JOB_NODELIST \"; SLURMD_NODENAME \" $SLURMD_NODENAME  \"; SLURM_JOB_NUM_NODES \" $SLURM_JOB_NUM_NODES\n",
    "python /lustre/-42ai/work/example/train.py\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submitted batch job 142\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sbatch -D /lustre/-42ai/work/example /lustre/-42ai/work/example/job.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID  142 ; SLURM_JOB_NAME  job.sh ; SLURM_JOB_NODELIST  slurm-worker-[1-5] ; SLURMD_NODENAME  slurm-worker-1 ; SLURM_JOB_NUM_NODES  5\r\n",
      "Parameter nodes:  ['slurm-worker-1']\r\n",
      "Worker nodes:  ['slurm-worker-2', 'slurm-worker-3', 'slurm-worker-4', 'slurm-worker-5']\r\n",
      "My job name:  ps\r\n",
      "My task index:  0\r\n",
      "Cluster:  {'worker': ['slurm-worker-2:2222', 'slurm-worker-3:2222', 'slurm-worker-4:2222', 'slurm-worker-5:2222'], 'ps': ['slurm-worker-1:2222']}\r\n",
      "Done!\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/example/example_1.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we immediately check whether the current job is a parameter server. Since all the work in a parameter server(ps) is handled by the tf.distribute.Server(which is running in a separate thread), we can just call server.join() and not execute the rest of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "\n",
    "if my_job_name == 'ps':\n",
    "    print(\"Current job is a parameter server. Sever started\")\n",
    "    server.join()\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/example/job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/example/job.sh\n",
    "#! /bin/bash\n",
    "#SBATCH --output=example_2.out              # Name of stdout output file\n",
    "#SBATCH --error=example_2.err             # Name of stderr error file\n",
    "#SBATCH --nodes=5                    # Total # of nodes\n",
    "#SBATCH --ntasks=5                   # Total # of mpi tasks\n",
    "#SBATCH --partition=debug                # Queue (partition) name\n",
    "#SBATCH --ntasks-per-node  1   # Number of tasks per node\n",
    "#SBATCH --mem=4G                 # total memory per node (default is 4 GB per CPU-core)\n",
    "#SBATCH --gres=gpu:1             # number of gpus per node\n",
    "#SBATCH --time=00:01:00            # Run time (hh:mm:ss)\n",
    "source activate /lustre/-42ai/work/new_conda_env\n",
    "\n",
    "echo \"SLURM_JOB_ID \" $SLURM_JOB_ID  \"; SLURM_JOB_NAME \" $SLURM_JOB_NAME \"; SLURM_JOB_NODELIST \" $SLURM_JOB_NODELIST \"; SLURMD_NODENAME \" $SLURMD_NODENAME  \"; SLURM_JOB_NUM_NODES \" $SLURM_JOB_NUM_NODES\n",
    "python /lustre/-42ai/work/example/train.py\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submitted batch job 145\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sbatch -D /lustre/-42ai/work/example /lustre/-42ai/work/example/job.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "WARNING:tensorflow:From /lustre/-42ai/work/example/train.py:85: The name tf.train.Server is deprecated. Please use tf.distribute.Server instead.\r\n",
      "\r\n",
      "2020-07-16 23:33:38.927284: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n",
      "2020-07-16 23:33:38.982667: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n",
      "2020-07-16 23:33:38.986062: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56485d732600 executing computations on platform Host. Devices:\r\n",
      "2020-07-16 23:33:38.986230: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n",
      "E0716 23:33:38.991652597    8923 socket_utils_common_posix.cc:198] check for SO_REUSEPORT: {\"created\":\"@1594942418.991603551\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":166}\r\n",
      "2020-07-16 23:33:38.992278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n",
      "2020-07-16 23:33:38.992356: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> slurm-worker-2:2222, 1 -> slurm-worker-3:2222, 2 -> slurm-worker-4:2222, 3 -> slurm-worker-5:2222}\r\n",
      "2020-07-16 23:33:38.999264: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2222\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/example/example_2.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see ther server is started now.\n",
    "\n",
    "\n",
    "The warning \"TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\" means the CPU's speed is not as fast as it could be. The warning doesn’t impact functionality for now but would effect potentially performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Placing the Variables on a parameter server\n",
    "These two functions are used when defining the model parameters. Note the 'with tf.device(\"/job:ps/task:0\")\" statements telling TensorFlow that the variables should be placed on the parameter server, thus enabling them to be shared between the workers. The \"0\" index denotes the I.D. of the parameter sever used to store the variable. Here we're only using one server, so all the variables are placed on task \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    with tf.device(\"/job:ps/task:0\"):\n",
    "        initial = tf.compat.v1.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    with tf.device(\"/job:ps/task:0\"):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a  /lustre/-42ai/work/example/train.py\n",
    "\n",
    "data_dir = '/lustre/-42ai/work/example/cifar-10-batches-py'\n",
    "# try the first two batches in this example\n",
    "filelist = [os.path.join(data_dir, 'data_batch_1'),\n",
    "            os.path.join(data_dir, 'data_batch_2')]\n",
    "data, labels = [], []\n",
    "\n",
    "for f in filelist:\n",
    "    with open(f, 'rb') as fo:\n",
    "        data_elem = pickle.load(fo, encoding='latin1')\n",
    "        data.append(data_elem['data'])\n",
    "        labels.extend(data_elem['labels'])\n",
    "data = np.vstack(d for d in data)\n",
    "print('data shape: ', data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "try:\n",
    "    print(my_task_index)\n",
    "    with tf.device('/job:worker/task:{}'.format(my_task_index)):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 3072], name='x')\n",
    "        y = tf.placeholder(tf.uint8, shape=[None, 1], name='y')\n",
    "        print(\"x,y\")\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    # FIRST CONVOLUTIONAL LAYER\n",
    "    y_one_hot = tf.one_hot(indices=y, depth=10)\n",
    "    \n",
    "    ks = 5\n",
    "    n_filters1 = 16\n",
    "    W_conv1 = weight_variable([ks, ks, 3, n_filters1])\n",
    "    b_conv1 = bias_variable([n_filters1])\n",
    "    \n",
    "    reshaped = tf.reshape(x, [-1, 3, 32, 32])\n",
    "    transposed = tf.transpose(reshaped, [0, 2, 3, 1])\n",
    "    x_image = (transposed - 128) / 128\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    # SECOND CONVOLUTIONAL LAYER\n",
    "    n_filters2 = 64\n",
    "    W_conv2 = weight_variable([ks, ks, n_filters1, n_filters2])\n",
    "    b_conv2 = bias_variable([n_filters2])\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    # FULLY CONNECTED LAYER\n",
    "    hidden_neurons = 512 \n",
    "    W_fc1 = weight_variable([5 * 5 * n_filters2, hidden_neurons])\n",
    "    b_fc1 = bias_variable([hidden_neurons])\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 5 * 5 * n_filters2])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # DROPOUT\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    # SOFTMAX\n",
    "    W_fc2 = weight_variable([hidden_neurons, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_one_hot)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    opt = tf.train.AdamOptimizer(1e-3)\n",
    "    opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=len(cluster['worker']),\n",
    "                                total_num_replicas=len(cluster['worker']))\n",
    "    global_step = bias_variable([])\n",
    "    train_step = opt.minimize(loss, global_step=global_step)\n",
    "    sync_replicas_hook = opt.make_session_run_hook(is_chief)\n",
    "    \n",
    "    y_hat = tf.round(tf.argmax(tf.nn.softmax(y_conv), 1))\n",
    "    y_hat = tf.cast(y_hat, tf.uint8)\n",
    "    y_hat = tf.reshape(y_hat, [-1, 1])\n",
    "    correct_prediction = tf.equal(y_hat, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"1\")\n",
    "    \n",
    "    x = tf.compat.v1.placeholder(tf.float32, shape=[None, 3072], name='x')\n",
    "    y = tf.compat.v1.placeholder(tf.uint8, shape=[None, 1], name='y')\n",
    "    \n",
    "    # FIRST CONVOLUTIONAL LAYER\n",
    "    y_one_hot = tf.one_hot(indices=y, depth=10)\n",
    "    \n",
    "    ks = 5\n",
    "    n_filters1 = 16\n",
    "    W_conv1 = weight_variable([ks, ks, 3, n_filters1])\n",
    "    b_conv1 = bias_variable([n_filters1])\n",
    "    \n",
    "    reshaped = tf.reshape(x, [-1, 3, 32, 32])\n",
    "    transposed = tf.transpose(reshaped, [0, 2, 3, 1])\n",
    "    x_image = (transposed - 128) / 128\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    # SECOND CONVOLUTIONAL LAYER\n",
    "    n_filters2 = 64\n",
    "    W_conv2 = weight_variable([ks, ks, n_filters1, n_filters2])\n",
    "    b_conv2 = bias_variable([n_filters2])\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    # FULLY CONNECTED LAYER\n",
    "    hidden_neurons = 512 \n",
    "    W_fc1 = weight_variable([5 * 5 * n_filters2, hidden_neurons])\n",
    "    b_fc1 = bias_variable([hidden_neurons])\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 5 * 5 * n_filters2])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # DROPOUT\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    # SOFTMAX\n",
    "    W_fc2 = weight_variable([hidden_neurons, 10])\n",
    "    b_fc2 = bias_variable([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "Instead of using the usual AdamOptimizer, we're wrapping it with the SyncReplicasOptimizer. This enables us to prevent the application of stale gradients. In distributed training, the network communication may introduce communication delays which make it harder to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "\n",
    "with tf.device('/job:worker/task:{}'.format(my_task_index)):\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_one_hot)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    opt = tf.compat.v1.train.AdamOptimizer(1e-3)\n",
    "    opt = tf.compat.v1.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=len(cluster['worker']),\n",
    "                                total_num_replicas=len(cluster['worker']))\n",
    "    global_step = bias_variable([])\n",
    "    train_step = opt.minimize(loss, global_step=global_step)\n",
    "    sync_replicas_hook = opt.make_session_run_hook(1)\n",
    "    \n",
    "    y_hat = tf.round(tf.argmax(tf.nn.softmax(y_conv), 1))\n",
    "    y_hat = tf.cast(y_hat, tf.uint8)\n",
    "    y_hat = tf.reshape(y_hat, [-1, 1])\n",
    "    correct_prediction = tf.equal(y_hat, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "\n",
    "def batch_generator(data, labels, batch_size=32):\n",
    "    x_batch, y_batch = [], []\n",
    "    for d, l in zip(data, labels):\n",
    "        x_batch.append(d)\n",
    "        y_batch.append(l)\n",
    "        if len(x_batch) == batch_size:\n",
    "            yield np.vstack(x_batch),np.vstack(y_batch)\n",
    "            x_batch = []\n",
    "            y_batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the session\n",
    "In distributed settings we're using the tf.train.MonitoredTrainingSession instead of the usual tf.Session. This ensures the variables are properly initialized. It also allows you to restore a previously saved model and control how summaries and checkpoints are written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "step = 0\n",
    "sess = tf.compat.v1.train.MonitoredTrainingSession(master=server.target, is_chief=1,\n",
    "                                         hooks=[sync_replicas_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to slurm to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /lustre/-42ai/work/example/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a /lustre/-42ai/work/example/train.py\n",
    "\n",
    "for i in range(epochs):\n",
    "    bg = batch_generator(data, labels, batch_size)\n",
    "    for j, (data_batch, label_batch) in enumerate(bg):\n",
    "        if (j+i) % len(cluster['worker']) != my_task_index:\n",
    "            continue\n",
    "        _, loss_, acc = sess.run([train_step, loss, accuracy],\n",
    "                                feed_dict={x: data_batch,\n",
    "                                          y: label_batch.reshape(-1,1),\n",
    "                                          keep_prob: 0.5})\n",
    "        step += 1\n",
    "        print(step, my_task_index, loss_, acc)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /lustre/-42ai/work/example/job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /lustre/-42ai/work/example/job.sh\n",
    "#! /bin/bash\n",
    "#SBATCH --output=example_3.out              # Name of stdout output file\n",
    "#SBATCH --error=example_3.err             # Name of stderr error file\n",
    "#SBATCH --nodes=5                    # Total # of nodes\n",
    "#SBATCH --ntasks=5                   # Total # of mpi tasks\n",
    "#SBATCH --partition=debug                # Queue (partition) name\n",
    "#SBATCH --ntasks-per-node  1   # Number of tasks per node\n",
    "#SBATCH --mem=4G                 # total memory per node (default is 4 GB per CPU-core)\n",
    "#SBATCH --gres=gpu:1             # number of gpus per node\n",
    "#SBATCH --time=12:00:00            # Run time (hh:mm:ss)\n",
    "source activate /lustre/-42ai/work/new_conda_env\n",
    "\n",
    "echo \"SLURM_JOB_ID \" $SLURM_JOB_ID  \"; SLURM_JOB_NAME \" $SLURM_JOB_NAME \"; SLURM_JOB_NODELIST \" $SLURM_JOB_NODELIST \"; SLURMD_NODENAME \" $SLURMD_NODENAME  \"; SLURM_JOB_NUM_NODES \" $SLURM_JOB_NUM_NODES\n",
    "python /lustre/-42ai/work/example/train.py\n",
    "echo \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submitted batch job 146\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sbatch -D /lustre/-42ai/work/example /lustre/-42ai/work/example/job.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID  146 ; SLURM_JOB_NAME  job.sh ; SLURM_JOB_NODELIST  slurm-worker-[1-5] ; SLURMD_NODENAME  slurm-worker-1 ; SLURM_JOB_NUM_NODES  5\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/example/example_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/lustre/-42ai/work/new_conda_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "WARNING:tensorflow:From /lustre/-42ai/work/example/train.py:85: The name tf.train.Server is deprecated. Please use tf.distribute.Server instead.\r\n",
      "\r\n",
      "2020-07-16 23:35:45.200106: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n",
      "2020-07-16 23:35:45.254579: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n",
      "2020-07-16 23:35:45.259961: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5612274bb000 executing computations on platform Host. Devices:\r\n",
      "2020-07-16 23:35:45.260129: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n",
      "E0716 23:35:45.265494003    9145 socket_utils_common_posix.cc:198] check for SO_REUSEPORT: {\"created\":\"@1594942545.265454395\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":166}\r\n",
      "2020-07-16 23:35:45.266120: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n",
      "2020-07-16 23:35:45.266201: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> slurm-worker-2:2222, 1 -> slurm-worker-3:2222, 2 -> slurm-worker-4:2222, 3 -> slurm-worker-5:2222}\r\n",
      "2020-07-16 23:35:45.272497: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2222\r\n"
     ]
    }
   ],
   "source": [
    "!cat /lustre/-42ai/work/example/example_3.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOBID</th>\n",
       "      <th>PARTITION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>USER</th>\n",
       "      <th>ST</th>\n",
       "      <th>TIME</th>\n",
       "      <th>NODES</th>\n",
       "      <th>NODELIST(REASON)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>debug</td>\n",
       "      <td>job.sh</td>\n",
       "      <td>jovyan</td>\n",
       "      <td>R</td>\n",
       "      <td>0:52</td>\n",
       "      <td>5</td>\n",
       "      <td>slurm-worker-[1-5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   JOBID PARTITION    NAME    USER ST  TIME  NODES    NODELIST(REASON)\n",
       "0    146     debug  job.sh  jovyan  R  0:52      5  slurm-worker-[1-5]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%squeue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
